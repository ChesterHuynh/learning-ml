{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "data = pd.read_csv('/Users/ChesterHuynh/learningml/Kaggle/Tutorials/house-prices-advanced-regression-techniques/train.csv')\n",
    "data.dropna(axis=0, subset=['SalePrice'], inplace=True) # get rid of entries with no corresponding target\n",
    "y = data.SalePrice\n",
    "X = data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.25)\n",
    "\n",
    "my_imputer = Imputer()\n",
    "X_train = my_imputer.fit_transform(X_train)\n",
    "X_test = my_imputer.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "my_model = XGBRegressor()\n",
    "# Add silent=True to avoid printing out updates with each cycle\n",
    "my_model.fit(X_train, y_train, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 19287.2768193\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "predictions = my_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost has a few parameters that can dramatically affect model's accuracy and training speed\n",
    "# n_estimators = specifies how many times to go through the modeling cycle\n",
    "# low n_est -> underfit, high n_est -> overfit\n",
    "# typical values range from 100-1000, but depends on learning rate\n",
    "# early_stopping_rounds = causes model to stop iterating when validating score stops improving\n",
    "# since validation score not improving once can occur due to random chance,\n",
    "# one needs to specify number of rounds of straight deterioration before stopping\n",
    "# early_stopping_rounds = 5 seems reasonable\n",
    "\n",
    "my_model = XGBRegressor(n_estimators=1000)\n",
    "my_model.fit(X_train, y_train, early_stopping_rounds=5,\n",
    "             eval_set = [(X_test, y_test)], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.05, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning_rate allows you to multiply predictions by a small number\n",
    "# rather than simply adding up predictions from each component model, which\n",
    "# reduces model's propensity to overfit, so that you can use higher n_est\n",
    "# small learning_rate (and large # of estimators) will yield more accurate models,\n",
    "# but may take longer to iterate through\n",
    "\n",
    "my_model = XGBRegressor(n_estimators=1000, learning_rate = 0.05)\n",
    "my_model.fit(X_train, y_train, early_stopping_rounds=5,\n",
    "             eval_set = [(X_test, y_test)], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n_jobs = number of jobs running in parallel for large datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
